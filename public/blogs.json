[
    {
        "_id": "659511412c5227e4dab829d3",
        "bloggerImage": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSon7UXXyxvoxfrD0brWchUB7kIU545JP7QtQ&usqp=CAU",
        "bloggerName": "Ridoy Khan",
        "publishingDate": "2023-11-08T16:57:21.994Z",
        "blogTitle": "There are 3 ways to stop forEach in JavaScript",
        "blogContent": "<p><img src=\"https://miro.medium.com/v2/resize:fit:700/0*MoaTV0fSycy_nxoD\"></p><p><br></p><p>Interviewer:&nbsp;<strong>Can you stop a forEach loop in JavaScript?</strong>&nbsp;This is a question I was once asked during an interview, and my initial response was,&nbsp;<strong>“No, I can’t do that.”</strong></p><p><strong>Unfortunately, my response led the interviewer to end the interview&nbsp;abruptly.</strong></p><p>Frustrated with the outcome, I asked the interviewer, “Why? Is it actually possible to stop a forEach loop in JavaScript?”</p><p>Before the interviewer could answer, I took a moment to explain my understanding of why we couldn’t directly stop a forEach loop in JavaScript.</p><h1>Is my answer correct?</h1><p>My friends, what numbers will be output by the following code?</p><p><br></p><p>Will it output just one number or more?</p><p>Yes, it will output ‘0’, ‘1’, ‘2’, ‘3’.</p><p>const array = [ -3, -2, -1, 0, 1, 2, 3 ]</p><p><br></p><p>array.forEach((it) =&gt; {</p><p>  if (it &gt;= 0) {</p><p>    console.log(it)</p><p>    return // or break</p><p>  }</p><p>})</p><p><img src=\"https://miro.medium.com/v2/resize:fit:700/0*mWZMmBVWaP4ut8Hg.png\"></p><p>That’s right! I showed this code to the interviewer, but&nbsp;<strong>he still believed that we could stop a forEach loop in JavaScript.</strong></p><p><img src=\"https://miro.medium.com/v2/resize:fit:700/0*p8xPlQcc9zJ5Wzcb\"></p><p><br></p><h1>Why?</h1><p>In order to convince him, I had to implement the&nbsp;forEach&nbsp;simulation again.</p><p><br></p><p>Array.prototype.forEach2 = function (callback, thisCtx) {</p><p>  if (typeof callback !== 'function') {</p><p>    throw `${callback} is not a function`</p><p>  }</p><p><br></p><p>  const length = this.length</p><p>  let i = 0</p><p><br></p><p>  while (i &lt; length) {</p><p>    if (this.hasOwnProperty(i)) {</p><p>      // Note here：Each callback function will be executed once</p><p>      callback.call(thisCtx, this[ i ], i, this)</p><p>    }</p><p>    i++</p><p>  }</p><p>}</p><p>Yes, when we use ‘forEach’ to iterate through an array, the callback will be executed once for each element of the array, and there is no way we can break out of it…</p><p><br></p>",
        "documnetReadingTime": 5
    },
    {
        "_id": "659512282c5227e4dab829e1",
        "bloggerImage":"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQH-blsvoWQeqmoTzZB3tISQMu2dw8R6TY4IA&usqp=CAU",
        "bloggerName": "Asma Khan",
        "blogCategory": "programming",
        "publishingDate": "2023-11-08T16:57:21.994Z",
        "blogTitle": "There are 3 ways to stop forEach in JavaScript",
        "blogContent": "<p><img src=\"https://miro.medium.com/v2/resize:fit:700/0*MoaTV0fSycy_nxoD\"></p><p>Photo by&nbsp;<a href=\"https://unsplash.com/@hhh13?utm_source=medium&amp;utm_medium=referral\" rel=\"noopener noreferrer\" target=\"_blank\">傅甬 华</a>&nbsp;on&nbsp;<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\" rel=\"noopener noreferrer\" target=\"_blank\">Unsplash</a></p><p>Interviewer:&nbsp;<strong>Can you stop a forEach loop in JavaScript?</strong>&nbsp;This is a question I was once asked during an interview, and my initial response was,&nbsp;<strong>“No, I can’t do that.”</strong></p><p><strong>Unfortunately, my response led the interviewer to end the interview&nbsp;abruptly.</strong></p><p>Frustrated with the outcome, I asked the interviewer, “Why? Is it actually possible to stop a forEach loop in JavaScript?”</p><p>Before the interviewer could answer, I took a moment to explain my understanding of why we couldn’t directly stop a forEach loop in JavaScript.</p><h1>Is my answer correct?</h1><p>My friends, what numbers will be output by the following code?</p><p><br></p><p>Will it output just one number or more?</p><p>Yes, it will output ‘0’, ‘1’, ‘2’, ‘3’.</p><p>const array = [ -3, -2, -1, 0, 1, 2, 3 ]</p><p><br></p><p>array.forEach((it) =&gt; {</p><p>  if (it &gt;= 0) {</p><p>    console.log(it)</p><p>    return // or break</p><p>  }</p><p>})</p><p><img src=\"https://miro.medium.com/v2/resize:fit:700/0*mWZMmBVWaP4ut8Hg.png\"></p><p>That’s right! I showed this code to the interviewer, but&nbsp;<strong>he still believed that we could stop a forEach loop in JavaScript.</strong></p><p><img src=\"https://miro.medium.com/v2/resize:fit:700/0*p8xPlQcc9zJ5Wzcb\"></p><p>Photo by&nbsp;<a href=\"https://unsplash.com/@christnerfurt?utm_source=medium&amp;utm_medium=referral\" rel=\"noopener noreferrer\" target=\"_blank\">Christian Erfurt</a>&nbsp;on&nbsp;<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\" rel=\"noopener noreferrer\" target=\"_blank\">Unsplash</a></p><p><strong>Oh my God, you must be joking.</strong></p><h1>Why?</h1><p>In order to convince him, I had to implement the&nbsp;forEach&nbsp;simulation again.</p><p><br></p><p>Array.prototype.forEach2 = function (callback, thisCtx) {</p><p>  if (typeof callback !== 'function') {</p><p>    throw `${callback} is not a function`</p><p>  }</p><p><br></p><p>  const length = this.length</p><p>  let i = 0</p><p><br></p><p>  while (i &lt; length) {</p><p>    if (this.hasOwnProperty(i)) {</p><p>      // Note here：Each callback function will be executed once</p><p>      callback.call(thisCtx, this[ i ], i, this)</p><p>    }</p><p>    i++</p><p>  }</p><p>}</p><p>Yes, when we use ‘forEach’ to iterate through an array, the callback will be executed once for each element of the array, and there is no way we can break out of it…</p><p><br></p>",
        "documnetReadingTime": 5
    },
    {
        "_id": "659518f12c5227e4dab82b7f",
        "bloggerImage":"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQH-blsvoWQeqmoTzZB3tISQMu2dw8R6TY4IA&usqp=CAU",
        "bloggerName": "Roton",
        "facebookLink": "www.dgfdsfgdjkg.com",
        "blogCategory": "web development",
        "publishingDate": "2023-11-08T16:57:21.994Z",
        "blogTitle": "Documentation and examples for Boosted typography, including global settings, headings, body text, lists, and more",
        "blogContent": "<h2><strong>Typography</strong></h2><p>Documentation and examples for Boosted typography, including global settings, headings, body text, lists, and more.</p><p><br></p><h2><strong>Global settings</strong></h2><p><span style=\"color: rgb(255, 153, 0);\">Boosted sets basic global display, typography, and link styles. When more control is needed, check out the&nbsp;</span><a href=\"https://boosted.orange.com/docs/4.6/utilities/text/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"background-color: transparent; color: rgb(255, 153, 0);\">textual utility classes</a><span style=\"color: rgb(255, 153, 0);\">.</span></p><ul><li><span style=\"color: rgb(255, 153, 0);\">Use&nbsp;</span><strong style=\"color: rgb(255, 153, 0);\">Helvetica Neue</strong><span style=\"color: rgb(255, 153, 0);\">&nbsp;(locally if available, webfont otherwise), with Helvetica and Arial as preferred fallback.</span></li><li><span style=\"color: rgb(255, 153, 0);\">Then use a&nbsp;</span><a href=\"https://boosted.orange.com/docs/4.6/content/reboot/#native-font-stack\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"background-color: transparent; color: rgb(255, 153, 0);\">native font stack</a><span style=\"color: rgb(255, 153, 0);\">&nbsp;that selects the best&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">font-family</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;for each OS and device.</span></li><li><span style=\"color: rgb(255, 153, 0);\">For a more inclusive and accessible type scale, we use the browser’s default root&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">font-size</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;(typically 16px) so visitors can customize their browser defaults as needed.</span></li><li><span style=\"color: rgb(255, 153, 0);\">Use the&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">$font-family-base</code><span style=\"color: rgb(255, 153, 0);\">,&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">$font-size-base</code><span style=\"color: rgb(255, 153, 0);\">,&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">$line-height-base</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;and&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">$letter-spacing-base</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;attributes as our typographic base applied to the&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">&lt;body&gt;</code><span style=\"color: rgb(255, 153, 0);\">.</span></li><li><span style=\"color: rgb(255, 153, 0);\">Set the global link color via&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">$link-color</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;and apply link underlines only on&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">:hover</code><span style=\"color: rgb(255, 153, 0);\">.</span></li><li><span style=\"color: rgb(255, 153, 0);\">Use&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">$body-bg</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;to set a&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">background-color</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;on the&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">&lt;body&gt;</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;(</span><code style=\"color: rgb(255, 153, 0);\">#fff</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;by default).</span></li></ul><h4><span style=\"color: rgb(255, 153, 0);\">These styles can be found within&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">_reboot.scss</code><span style=\"color: rgb(255, 153, 0);\">, and the global variables are defined in&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">_variables.scss</code><span style=\"color: rgb(255, 153, 0);\">. Make sure to set&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">$font-size-base</code><span style=\"color: rgb(255, 153, 0);\">&nbsp;in&nbsp;</span><code style=\"color: rgb(255, 153, 0);\">rem</code><span style=\"color: rgb(255, 153, 0);\">.</span></h4><p><br></p><h2><strong style=\"color: rgb(102, 102, 0);\">Abbreviations</strong></h2><h4><span style=\"color: rgb(0, 102, 204);\">Stylized implementation of HTML’s&nbsp;</span><code style=\"color: rgb(0, 102, 204);\">&lt;abbr&gt;</code><span style=\"color: rgb(0, 102, 204);\">&nbsp;element for abbreviations and acronyms to show the expanded version on hover. Abbreviations have a default underline and gain a help cursor to provide additional context on hover and to users of assistive technologies.</span></h4><h4><span style=\"color: rgb(0, 102, 204);\">Add&nbsp;</span><code style=\"color: rgb(0, 102, 204);\">.initialism</code><span style=\"color: rgb(0, 102, 204);\">&nbsp;to an abbreviation for a slightly smaller font-size.</span></h4><h4><span style=\"color: rgb(0, 102, 204);\">attr</span></h4>",
        "documnetReadingTime": 4
    },
    {
        "_id": "65951ced2c5227e4dab82bcc",
        "bloggerImage":"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQH-blsvoWQeqmoTzZB3tISQMu2dw8R6TY4IA&usqp=CAU",
        "bloggerName": "Asma Khan",
        "blogCategory": "programming",
        "publishingDate": "2023-11-08T16:57:21.994Z",
        "blogTitle": "React Js Clean Code Guide",
        "blogContent": "<p><img src=\"https://miro.medium.com/v2/resize:fit:700/1*nX3dW9vnOTDLGjlfRZShtQ.png\" height=\"394\" width=\"700\"></p><p>React.js has revolutionized front-end development by providing a powerful and efficient way to build user interfaces. However, as your React application grows in complexity, maintaining clean, readable, and maintainable code becomes&nbsp;crucial. In this guide, we’ll explore best practices for writing clean React.js code that not only works but is also easy to understand and maintain.</p><p>Originally published at-&nbsp;<a href=\"https://javascript.withcodeexample.com/blog/react-js-clean-code-best-practices/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: inherit;\">https://javascript.withcodeexample.com/blog/react-js-clean-code-best-practices/</a></p><p><a href=\"https://twitter.com/harendraverma2\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: inherit;\"><img src=\"https://miro.medium.com/v2/resize:fit:700/1*C44b1m2UzUdQtFF2eNdpMQ.png\" alt=\"follow me on twitter\" height=\"118\" width=\"700\"></a></p><h2>1. Component Structure and Organization</h2><p>A well-structured component hierarchy simplifies navigation and aids in understanding the flow of your application. Follow these guidelines:</p><p><br></p><ul><li><strong>Single Responsibility Principle (SRP)</strong>: Keep your components focused on a single responsibility. If a component becomes too large, consider breaking it down into smaller, more manageable pieces.</li><li><strong>Container and Presentation Components</strong>: Separate your components into two categories: container components (responsible for data and logic) and presentation components (focused on rendering UI). This separation enhances code clarity and reusability.</li><li><strong>Folder Structure</strong>: Organize your components in a logical folder structure. Group related components together, and consider using directories like&nbsp;<code style=\"background-color: rgb(242, 242, 242);\">components</code>,&nbsp;<code style=\"background-color: rgb(242, 242, 242);\">containers</code>, and&nbsp;<code style=\"background-color: rgb(242, 242, 242);\">utils</code>&nbsp;to keep things organized.</li></ul><h2>2. Descriptive Naming</h2><p>Descriptive naming in React refers to the practice of choosing meaningful and self-explanatory names for variables, functions, components, files, and other elements within your React application’s codebase. The goal of descriptive naming is to make the purpose and functionality of each element immediately clear to other developers who read or collaborate on the code. This practice enhances code readability, maintainability, and overall understanding of the application.</p><p><br></p><p>Here are some guidelines for using descriptive naming in your React code:</p><p><strong>Components</strong>: Choose component names that accurately represent their purpose or role within the application. Use descriptive nouns or noun phrases that clearly communicate what the component does.</p><p><br></p><pre class=\"ql-syntax\" spellcheck=\"false\">// Good: Descriptive component name\n&lt;UserProfile /&gt;\n\n// Avoid: Vague or unclear component name\n&lt;Component1 /&gt;\n</pre><p><br></p><p><strong style=\"color: rgb(36, 36, 36);\">Variables and Functions</strong><span style=\"color: rgb(36, 36, 36);\">: Opt for variable and function names that explain their purpose or the data they hold/manipulate. Use camelCase for variables and lowercase letters for functions.</span></p><p><br></p><pre class=\"ql-syntax\" spellcheck=\"false\">\n\n\n// Good: Descriptive variable and function names\n\nconst userDisplayName = 'John Doe';\n\n\n\n\nfunction calculateTotalPrice(items) {\n\n  // ...\n\n}\n\n\n\n\n// Avoid: Unclear or abbreviated names\n\nconst uName = 'John Doe';\n\n\n\n\nfunction calc(items) {\n\n  // ...\n\n}\n</pre>",
        "documnetReadingTime": 4
    },
    {
        "_id": "65952e1ea8325fed4f80cc96",
        "bloggerImage":"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQH-blsvoWQeqmoTzZB3tISQMu2dw8R6TY4IA&usqp=CAU",
        "bloggerName": "Ali Mollah",
        "blogCategory": "programming",
        "publishingDate": "2023-11-08T16:57:21.994Z",
        "blogTitle": "Learn javascript",
        "blogContent": "<ol><li>Removed the negative sign before the values of <code style=\"color: var(--tw-prose-code);\">facebook</code>, <code style=\"color: var(--tw-prose-code);\">github</code>, and <code style=\"color: var(--tw-prose-code);\">linkedin</code> variables in the <code style=\"color: var(--tw-prose-code);\">handlePostSubmission</code> function.</li><li>Corrected how the selected value is retrieved from the dropdown in <code style=\"color: var(--tw-prose-code);\">blogCategory</code>.</li><li>Changed the <code style=\"color: var(--tw-prose-code);\">documentReadingTime</code> property name to match the one used in the backend (<code style=\"color: var(--tw-prose-code);\">documnetReadingTime</code> to <code style=\"color: var(--tw-prose-code);\">documentReadingTime</code>).</li><li>Used <code style=\"color: var(--tw-prose-code);\">user.displayName</code> as the value for <code style=\"color: var(--tw-prose-code);\">bloggerName</code> assuming you want to use the logged-in user's display name. Adjust it according to your authentication setup.</li></ol><p><br></p>",
        "documnetReadingTime": 1
    },
    {
        "_id": "65952f1ba8325fed4f80cc98",
        "bloggerName": "Shawon Mandal",
        "bloggerImage":"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQH-blsvoWQeqmoTzZB3tISQMu2dw8R6TY4IA&usqp=CAU",
        "blogCategory": "machine learning",
        "publishingDate": "2023-11-08T16:57:21.994Z",
        "blogTitle": "Logistic Regression for Image Classification Using OpenCV",
        "blogContent": "<p class=\"ql-align-justify\">In a&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we explored logistic regression as a simple but popular machine learning algorithm for binary classification implemented in the OpenCV library.</p><p class=\"ql-align-justify\">So far, we have seen how logistic regression may be applied to a custom two-class dataset we have generated ourselves.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">In this tutorial, you will learn how the standard logistic regression algorithm, inherently designed for binary classification, can be modified to cater to multi-class classification problems by applying it to an image classification task.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">After completing this tutorial, you will know:</p><ul><li class=\"ql-align-justify\">Several of the most important characteristics of the logistic regression algorithm.</li><li class=\"ql-align-justify\">How the logistic regression algorithm can be modified for multi-class classification problems.<span style=\"background-color: initial;\">&nbsp;</span></li><li class=\"ql-align-justify\">How to apply logistic regression to the problem of image classification.<span style=\"background-color: initial;\">&nbsp;</span></li></ul><p class=\"ql-align-justify\">Let’s get started.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2023/12/logistic_multi_cover-scaled.jpg\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\"><img src=\"https://machinelearningmastery.com/wp-content/uploads/2023/12/logistic_multi_cover-1024x680.jpg\" height=\"532\" width=\"801\"></a></p><p class=\"ql-align-justify\">Logistic Regression for Image Classification Using OpenCV</p><p class=\"ql-align-justify\">Photo by&nbsp;<a href=\"https://unsplash.com/photos/landscape-photography-of-mountain-hit-by-sun-rays-78A265wPiO4\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">David Marcu</a>, some rights reserved.</p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Tutorial Overview</strong></h2><p class=\"ql-align-justify\">This tutorial is divided into three parts; they are:</p><ul><li class=\"ql-align-justify\">Recap of What Logistic Regression Is</li><li class=\"ql-align-justify\">Modifying Logistic Regression for Multi-Class Classification Problems</li><li class=\"ql-align-justify\">Applying Logistic Regression to a Multi-Class Classification Problem</li></ul><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Recap of What Logistic Regression Is</strong></h2><p class=\"ql-align-justify\">In a&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we started exploring OpenCV’s implementation of the logistic regression algorithm. So far, we have applied it to a custom two-class dataset that we have generated, consisting of two-dimensional points gathered into two clusters.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Following Jason Brownlee’s tutorials on logistic regression, we have also recapped the important points about logistic regression. We have seen that logistic regression is closely related to linear regression because they both involve a linear combination of features in generating a real-valued output. However, logistic regression extends this process by applying the logistic (or sigmoid) function. Hence its name. It is to map the real-valued output into a probability value within a range [0, 1]. This probability value is then classified as belonging to the default class if it exceeds a threshold of 0.5; otherwise, it is classified as belonging to the non-default class. This makes logistic regression inherently a method for&nbsp;<em style=\"background-color: initial;\">binary</em>&nbsp;classification.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">The logistic regression model is represented by as many coefficients as features in the input data, plus an extra bias value. These coefficients and bias values are learned during training using gradient descent or maximum likelihood estimation (MLE) techniques.</p><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Modifying Logistic Regression for Multi-Class Classification Problems</strong></h2><p class=\"ql-align-justify\">As mentioned in the previous section, the standard logistic regression method caters solely to two-class problems by how the logistic function and the ensuing thresholding process map the real-valued output of the linear combination of features into either class 0 or class 1.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Hence, catering for multi-class classification problems (or problems that involve more than two classes) with logistic regression requires modification of the standard algorithm.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">One technique to achieve this involves splitting the multi-class classification problem into multiple binary (or two-class) classification subproblems. The standard logistic regression method can then be applied to each subproblem. This is how OpenCV implements multi-class logistic regression:</p><blockquote class=\"ql-align-justify\"><em style=\"background-color: initial;\">… Logistic Regression supports both binary and multi-class classifications (for multi-class it creates a multiple 2-class classifiers).</em></blockquote><blockquote class=\"ql-align-justify\"><em style=\"background-color: initial;\">–&nbsp;</em><a href=\"https://docs.opencv.org/3.4/dc/dd6/ml_intro.html#ml_intro_lr\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"background-color: initial; color: rgb(66, 139, 202);\"><em>Logistic Regression, OpenCV</em></a></blockquote><p class=\"ql-align-justify\">A technique of this type is known as the&nbsp;<em style=\"background-color: initial;\">one-vs-one</em>&nbsp;approach, which involves training a separate binary classifier for each unique pair of classes in the dataset. During prediction, each of these binary classifiers votes for one of the two classes on which it was trained, and the class that receives the most votes across all classifiers is taken to be the predicted class.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">There are other techniques to achieve multi-class classification with logistic regression, such as through the&nbsp;<em style=\"background-color: initial;\">one-vs-rest</em>&nbsp;approach. You may find further information in these tutorials [<a href=\"https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">1</a>,&nbsp;<a href=\"https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">2</a>].</p><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Applying Logistic Regression to a Multi-Class Classification Problem</strong></h2><p class=\"ql-align-justify\">For this purpose, we shall be using the&nbsp;<a href=\"https://machinelearningmastery.com/?p=14607&amp;preview=true\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">digits dataset in OpenCV</a>, although the code we will develop may also be applied to other multi-class datasets.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Our first step is to load the OpenCV digits image, divide it into its many sub-images that feature handwritten digits from 0 to 9, and create their corresponding ground truth labels that will enable us to quantify the accuracy of the trained logistic regression model later. For this particular example, we will allocate 80% of the dataset images to the training set and the remaining 20% of the images to the testing set:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Load the digits image</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">img</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">sub_imgs</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">split_images</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: green; background-color: initial;\">'Images/digits.png'</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">20</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">&nbsp;</p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Obtain training and testing datasets from the digits image</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_labels</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_test_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_test_labels</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">split_data</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">20</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">sub_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">0.8</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">Next, we shall follow a process similar to what we did in the&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, where we trained and tested the logistic regression algorithm on a two-class dataset, changing a few parameters to adapt it to a larger multi-class dataset.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">The first step is, again, to create the logistic regression model itself:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Create an empty logistic regression model</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">LogisticRegression_create</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">()</span></p><p class=\"ql-align-justify\">We may, again, confirm that OpenCV implements Batch Gradient Descent as its default training method (represented by a value of 0) and then proceed to change this to a Mini-Batch Gradient Descent method, specifying the mini-batch size:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Check the default training method</span></p><p class=\"ql-align-justify\"><span style=\"color: purple; background-color: initial;\">print</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: green; background-color: initial;\">'Training Method:'</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">getTrainMethod</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">())</span></p><p class=\"ql-align-justify\">&nbsp;</p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Set the training method to Mini-Batch Gradient Descent and the size of the mini-batch</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setTrainMethod</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">LogisticRegression_MINI_BATCH</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setMiniBatchSize</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">400</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">Different mini-batch sizes will certainly affect the model’s training and prediction accuracy.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Our choice for the mini-batch size in this example was based on a heuristic approach for practicality, whereby a few mini-batch sizes were experimented with, and a value that resulted in a sufficiently high prediction accuracy (as we will see later) was identified. However, you should follow a more systematic approach, which can provide you with a more informed decision about the mini-batch size that offers a better compromise between computational cost and prediction accuracy for the task at hand.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Next, we shall define the number of iterations that we want to run the chosen training algorithm for before it terminates:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Set the number of iterations</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setIterations</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">10</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">We’re now set to train the logistic regression model on the training data:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Train the logistic regressor on the set of training data</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">train</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">astype</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">float32</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">),</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ROW_SAMPLE</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_labels</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">astype</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">float32</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">))</span></p><p class=\"ql-align-justify\">In our&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we printed out the learned coefficients to discover how the model, which best separated the two-class samples we worked with, was defined.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">We shall not be printing out the learned coefficients this time round, mainly because there are too many of them, given that we are now working with input data of higher dimensionality.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">What we shall alternatively do is print out the number of learned coefficients (rather t</p><p><br></p>",
        "documnetReadingTime": 12
    },
    {
        "_id": "65952f1ba8325fed4f80cc77",
        "bloggerName": "Nitu Mandal",
        "bloggerImage":"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQH-blsvoWQeqmoTzZB3tISQMu2dw8R6TY4IA&usqp=CAU",
        "blogCategory": "machine learning",
        "publishingDate": "2023-11-08T16:57:21.994Z",
        "blogTitle": "Logistic Regression for Image Classification Using OpenCV",
        "blogContent": "<p class=\"ql-align-justify\">In a&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we explored logistic regression as a simple but popular machine learning algorithm for binary classification implemented in the OpenCV library.</p><p class=\"ql-align-justify\">So far, we have seen how logistic regression may be applied to a custom two-class dataset we have generated ourselves.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">In this tutorial, you will learn how the standard logistic regression algorithm, inherently designed for binary classification, can be modified to cater to multi-class classification problems by applying it to an image classification task.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">After completing this tutorial, you will know:</p><ul><li class=\"ql-align-justify\">Several of the most important characteristics of the logistic regression algorithm.</li><li class=\"ql-align-justify\">How the logistic regression algorithm can be modified for multi-class classification problems.<span style=\"background-color: initial;\">&nbsp;</span></li><li class=\"ql-align-justify\">How to apply logistic regression to the problem of image classification.<span style=\"background-color: initial;\">&nbsp;</span></li></ul><p class=\"ql-align-justify\">Let’s get started.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2023/12/logistic_multi_cover-scaled.jpg\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\"><img src=\"https://machinelearningmastery.com/wp-content/uploads/2023/12/logistic_multi_cover-1024x680.jpg\" height=\"532\" width=\"801\"></a></p><p class=\"ql-align-justify\">Logistic Regression for Image Classification Using OpenCV</p><p class=\"ql-align-justify\">Photo by&nbsp;<a href=\"https://unsplash.com/photos/landscape-photography-of-mountain-hit-by-sun-rays-78A265wPiO4\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">David Marcu</a>, some rights reserved.</p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Tutorial Overview</strong></h2><p class=\"ql-align-justify\">This tutorial is divided into three parts; they are:</p><ul><li class=\"ql-align-justify\">Recap of What Logistic Regression Is</li><li class=\"ql-align-justify\">Modifying Logistic Regression for Multi-Class Classification Problems</li><li class=\"ql-align-justify\">Applying Logistic Regression to a Multi-Class Classification Problem</li></ul><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Recap of What Logistic Regression Is</strong></h2><p class=\"ql-align-justify\">In a&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we started exploring OpenCV’s implementation of the logistic regression algorithm. So far, we have applied it to a custom two-class dataset that we have generated, consisting of two-dimensional points gathered into two clusters.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Following Jason Brownlee’s tutorials on logistic regression, we have also recapped the important points about logistic regression. We have seen that logistic regression is closely related to linear regression because they both involve a linear combination of features in generating a real-valued output. However, logistic regression extends this process by applying the logistic (or sigmoid) function. Hence its name. It is to map the real-valued output into a probability value within a range [0, 1]. This probability value is then classified as belonging to the default class if it exceeds a threshold of 0.5; otherwise, it is classified as belonging to the non-default class. This makes logistic regression inherently a method for&nbsp;<em style=\"background-color: initial;\">binary</em>&nbsp;classification.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">The logistic regression model is represented by as many coefficients as features in the input data, plus an extra bias value. These coefficients and bias values are learned during training using gradient descent or maximum likelihood estimation (MLE) techniques.</p><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Modifying Logistic Regression for Multi-Class Classification Problems</strong></h2><p class=\"ql-align-justify\">As mentioned in the previous section, the standard logistic regression method caters solely to two-class problems by how the logistic function and the ensuing thresholding process map the real-valued output of the linear combination of features into either class 0 or class 1.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Hence, catering for multi-class classification problems (or problems that involve more than two classes) with logistic regression requires modification of the standard algorithm.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">One technique to achieve this involves splitting the multi-class classification problem into multiple binary (or two-class) classification subproblems. The standard logistic regression method can then be applied to each subproblem. This is how OpenCV implements multi-class logistic regression:</p><blockquote class=\"ql-align-justify\"><em style=\"background-color: initial;\">… Logistic Regression supports both binary and multi-class classifications (for multi-class it creates a multiple 2-class classifiers).</em></blockquote><blockquote class=\"ql-align-justify\"><em style=\"background-color: initial;\">–&nbsp;</em><a href=\"https://docs.opencv.org/3.4/dc/dd6/ml_intro.html#ml_intro_lr\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"background-color: initial; color: rgb(66, 139, 202);\"><em>Logistic Regression, OpenCV</em></a></blockquote><p class=\"ql-align-justify\">A technique of this type is known as the&nbsp;<em style=\"background-color: initial;\">one-vs-one</em>&nbsp;approach, which involves training a separate binary classifier for each unique pair of classes in the dataset. During prediction, each of these binary classifiers votes for one of the two classes on which it was trained, and the class that receives the most votes across all classifiers is taken to be the predicted class.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">There are other techniques to achieve multi-class classification with logistic regression, such as through the&nbsp;<em style=\"background-color: initial;\">one-vs-rest</em>&nbsp;approach. You may find further information in these tutorials [<a href=\"https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">1</a>,&nbsp;<a href=\"https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">2</a>].</p><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Applying Logistic Regression to a Multi-Class Classification Problem</strong></h2><p class=\"ql-align-justify\">For this purpose, we shall be using the&nbsp;<a href=\"https://machinelearningmastery.com/?p=14607&amp;preview=true\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">digits dataset in OpenCV</a>, although the code we will develop may also be applied to other multi-class datasets.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Our first step is to load the OpenCV digits image, divide it into its many sub-images that feature handwritten digits from 0 to 9, and create their corresponding ground truth labels that will enable us to quantify the accuracy of the trained logistic regression model later. For this particular example, we will allocate 80% of the dataset images to the training set and the remaining 20% of the images to the testing set:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Load the digits image</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">img</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">sub_imgs</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">split_images</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: green; background-color: initial;\">'Images/digits.png'</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">20</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">&nbsp;</p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Obtain training and testing datasets from the digits image</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_labels</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_test_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_test_labels</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">split_data</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">20</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">sub_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">0.8</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">Next, we shall follow a process similar to what we did in the&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, where we trained and tested the logistic regression algorithm on a two-class dataset, changing a few parameters to adapt it to a larger multi-class dataset.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">The first step is, again, to create the logistic regression model itself:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Create an empty logistic regression model</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">LogisticRegression_create</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">()</span></p><p class=\"ql-align-justify\">We may, again, confirm that OpenCV implements Batch Gradient Descent as its default training method (represented by a value of 0) and then proceed to change this to a Mini-Batch Gradient Descent method, specifying the mini-batch size:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Check the default training method</span></p><p class=\"ql-align-justify\"><span style=\"color: purple; background-color: initial;\">print</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: green; background-color: initial;\">'Training Method:'</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">getTrainMethod</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">())</span></p><p class=\"ql-align-justify\">&nbsp;</p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Set the training method to Mini-Batch Gradient Descent and the size of the mini-batch</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setTrainMethod</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">LogisticRegression_MINI_BATCH</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setMiniBatchSize</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">400</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">Different mini-batch sizes will certainly affect the model’s training and prediction accuracy.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Our choice for the mini-batch size in this example was based on a heuristic approach for practicality, whereby a few mini-batch sizes were experimented with, and a value that resulted in a sufficiently high prediction accuracy (as we will see later) was identified. However, you should follow a more systematic approach, which can provide you with a more informed decision about the mini-batch size that offers a better compromise between computational cost and prediction accuracy for the task at hand.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Next, we shall define the number of iterations that we want to run the chosen training algorithm for before it terminates:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Set the number of iterations</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setIterations</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">10</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">We’re now set to train the logistic regression model on the training data:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Train the logistic regressor on the set of training data</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">train</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">astype</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">float32</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">),</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ROW_SAMPLE</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_labels</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">astype</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">float32</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">))</span></p><p class=\"ql-align-justify\">In our&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we printed out the learned coefficients to discover how the model, which best separated the two-class samples we worked with, was defined.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">We shall not be printing out the learned coefficients this time round, mainly because there are too many of them, given that we are now working with input data of higher dimensionality.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">What we shall alternatively do is print out the number of learned coefficients (rather t</p><p><br></p>",
        "documnetReadingTime": 12
    },
    {
        "_id": "65952f1ba8325fed4f80cc71",
        "bloggerName": "Nitu Mandal",
        "bloggerImage":"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQH-blsvoWQeqmoTzZB3tISQMu2dw8R6TY4IA&usqp=CAU",
        "blogCategory": "machine learning",
        "publishingDate": "2023-11-08T16:57:21.994Z",
        "blogTitle": "Logistic Regression for Image Classification Using OpenCV",
        "blogContent": "<p class=\"ql-align-justify\">In a&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we explored logistic regression as a simple but popular machine learning algorithm for binary classification implemented in the OpenCV library.</p><p class=\"ql-align-justify\">So far, we have seen how logistic regression may be applied to a custom two-class dataset we have generated ourselves.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">In this tutorial, you will learn how the standard logistic regression algorithm, inherently designed for binary classification, can be modified to cater to multi-class classification problems by applying it to an image classification task.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">After completing this tutorial, you will know:</p><ul><li class=\"ql-align-justify\">Several of the most important characteristics of the logistic regression algorithm.</li><li class=\"ql-align-justify\">How the logistic regression algorithm can be modified for multi-class classification problems.<span style=\"background-color: initial;\">&nbsp;</span></li><li class=\"ql-align-justify\">How to apply logistic regression to the problem of image classification.<span style=\"background-color: initial;\">&nbsp;</span></li></ul><p class=\"ql-align-justify\">Let’s get started.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\"><a href=\"https://machinelearningmastery.com/wp-content/uploads/2023/12/logistic_multi_cover-scaled.jpg\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\"><img src=\"https://machinelearningmastery.com/wp-content/uploads/2023/12/logistic_multi_cover-1024x680.jpg\" height=\"532\" width=\"801\"></a></p><p class=\"ql-align-justify\">Logistic Regression for Image Classification Using OpenCV</p><p class=\"ql-align-justify\">Photo by&nbsp;<a href=\"https://unsplash.com/photos/landscape-photography-of-mountain-hit-by-sun-rays-78A265wPiO4\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">David Marcu</a>, some rights reserved.</p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Tutorial Overview</strong></h2><p class=\"ql-align-justify\">This tutorial is divided into three parts; they are:</p><ul><li class=\"ql-align-justify\">Recap of What Logistic Regression Is</li><li class=\"ql-align-justify\">Modifying Logistic Regression for Multi-Class Classification Problems</li><li class=\"ql-align-justify\">Applying Logistic Regression to a Multi-Class Classification Problem</li></ul><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Recap of What Logistic Regression Is</strong></h2><p class=\"ql-align-justify\">In a&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we started exploring OpenCV’s implementation of the logistic regression algorithm. So far, we have applied it to a custom two-class dataset that we have generated, consisting of two-dimensional points gathered into two clusters.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Following Jason Brownlee’s tutorials on logistic regression, we have also recapped the important points about logistic regression. We have seen that logistic regression is closely related to linear regression because they both involve a linear combination of features in generating a real-valued output. However, logistic regression extends this process by applying the logistic (or sigmoid) function. Hence its name. It is to map the real-valued output into a probability value within a range [0, 1]. This probability value is then classified as belonging to the default class if it exceeds a threshold of 0.5; otherwise, it is classified as belonging to the non-default class. This makes logistic regression inherently a method for&nbsp;<em style=\"background-color: initial;\">binary</em>&nbsp;classification.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">The logistic regression model is represented by as many coefficients as features in the input data, plus an extra bias value. These coefficients and bias values are learned during training using gradient descent or maximum likelihood estimation (MLE) techniques.</p><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Modifying Logistic Regression for Multi-Class Classification Problems</strong></h2><p class=\"ql-align-justify\">As mentioned in the previous section, the standard logistic regression method caters solely to two-class problems by how the logistic function and the ensuing thresholding process map the real-valued output of the linear combination of features into either class 0 or class 1.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Hence, catering for multi-class classification problems (or problems that involve more than two classes) with logistic regression requires modification of the standard algorithm.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">One technique to achieve this involves splitting the multi-class classification problem into multiple binary (or two-class) classification subproblems. The standard logistic regression method can then be applied to each subproblem. This is how OpenCV implements multi-class logistic regression:</p><blockquote class=\"ql-align-justify\"><em style=\"background-color: initial;\">… Logistic Regression supports both binary and multi-class classifications (for multi-class it creates a multiple 2-class classifiers).</em></blockquote><blockquote class=\"ql-align-justify\"><em style=\"background-color: initial;\">–&nbsp;</em><a href=\"https://docs.opencv.org/3.4/dc/dd6/ml_intro.html#ml_intro_lr\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"background-color: initial; color: rgb(66, 139, 202);\"><em>Logistic Regression, OpenCV</em></a></blockquote><p class=\"ql-align-justify\">A technique of this type is known as the&nbsp;<em style=\"background-color: initial;\">one-vs-one</em>&nbsp;approach, which involves training a separate binary classifier for each unique pair of classes in the dataset. During prediction, each of these binary classifiers votes for one of the two classes on which it was trained, and the class that receives the most votes across all classifiers is taken to be the predicted class.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">There are other techniques to achieve multi-class classification with logistic regression, such as through the&nbsp;<em style=\"background-color: initial;\">one-vs-rest</em>&nbsp;approach. You may find further information in these tutorials [<a href=\"https://machinelearningmastery.com/multinomial-logistic-regression-with-python/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">1</a>,&nbsp;<a href=\"https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">2</a>].</p><p class=\"ql-align-justify\"><br></p><h2 class=\"ql-align-justify\"><strong style=\"background-color: initial;\">Applying Logistic Regression to a Multi-Class Classification Problem</strong></h2><p class=\"ql-align-justify\">For this purpose, we shall be using the&nbsp;<a href=\"https://machinelearningmastery.com/?p=14607&amp;preview=true\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">digits dataset in OpenCV</a>, although the code we will develop may also be applied to other multi-class datasets.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Our first step is to load the OpenCV digits image, divide it into its many sub-images that feature handwritten digits from 0 to 9, and create their corresponding ground truth labels that will enable us to quantify the accuracy of the trained logistic regression model later. For this particular example, we will allocate 80% of the dataset images to the training set and the remaining 20% of the images to the testing set:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Load the digits image</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">img</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">sub_imgs</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">split_images</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: green; background-color: initial;\">'Images/digits.png'</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">20</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">&nbsp;</p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Obtain training and testing datasets from the digits image</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_labels</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_test_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_test_labels</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">split_data</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">20</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">sub_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">0.8</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">Next, we shall follow a process similar to what we did in the&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, where we trained and tested the logistic regression algorithm on a two-class dataset, changing a few parameters to adapt it to a larger multi-class dataset.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">The first step is, again, to create the logistic regression model itself:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Create an empty logistic regression model</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> = </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">LogisticRegression_create</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">()</span></p><p class=\"ql-align-justify\">We may, again, confirm that OpenCV implements Batch Gradient Descent as its default training method (represented by a value of 0) and then proceed to change this to a Mini-Batch Gradient Descent method, specifying the mini-batch size:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Check the default training method</span></p><p class=\"ql-align-justify\"><span style=\"color: purple; background-color: initial;\">print</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: green; background-color: initial;\">'Training Method:'</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">getTrainMethod</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">())</span></p><p class=\"ql-align-justify\">&nbsp;</p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Set the training method to Mini-Batch Gradient Descent and the size of the mini-batch</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setTrainMethod</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">LogisticRegression_MINI_BATCH</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setMiniBatchSize</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">400</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">Different mini-batch sizes will certainly affect the model’s training and prediction accuracy.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Our choice for the mini-batch size in this example was based on a heuristic approach for practicality, whereby a few mini-batch sizes were experimented with, and a value that resulted in a sufficiently high prediction accuracy (as we will see later) was identified. However, you should follow a more systematic approach, which can provide you with a more informed decision about the mini-batch size that offers a better compromise between computational cost and prediction accuracy for the task at hand.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">Next, we shall define the number of iterations that we want to run the chosen training algorithm for before it terminates:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Set the number of iterations</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">setIterations</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(206, 0, 0); background-color: initial;\">10</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">)</span></p><p class=\"ql-align-justify\">We’re now set to train the logistic regression model on the training data:</p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><br></p><p class=\"ql-align-justify\"><span style=\"color: rgb(255, 128, 0); background-color: initial;\"># Train the logistic regressor on the set of training data</span></p><p class=\"ql-align-justify\"><span style=\"color: rgb(0, 45, 122); background-color: initial;\">lr_digits</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">train</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_imgs</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">astype</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">float32</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">),</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ml</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">ROW_SAMPLE</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">,</span><span style=\"color: rgb(0, 111, 224); background-color: initial;\"> </span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">digits_train_labels</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">.</span><span style=\"color: rgb(0, 78, 208); background-color: initial;\">astype</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">(</span><span style=\"color: rgb(0, 45, 122); background-color: initial;\">float32</span><span style=\"color: rgb(51, 51, 51); background-color: initial;\">))</span></p><p class=\"ql-align-justify\">In our&nbsp;<a href=\"https://machinelearningmastery.com/logistic-regression-in-opencv/\" rel=\"noopener noreferrer\" target=\"_blank\" style=\"color: rgb(66, 139, 202); background-color: initial;\">previous tutorial</a>, we printed out the learned coefficients to discover how the model, which best separated the two-class samples we worked with, was defined.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">We shall not be printing out the learned coefficients this time round, mainly because there are too many of them, given that we are now working with input data of higher dimensionality.<span style=\"background-color: initial;\">&nbsp;</span></p><p class=\"ql-align-justify\">What we shall alternatively do is print out the number of learned coefficients (rather t</p><p><br></p>",
        "documnetReadingTime": 12
    }
]